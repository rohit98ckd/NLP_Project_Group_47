# -*- coding: utf-8 -*-
"""NLP_messho_review.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SxR-_fYkTWzumY_qF19SobXUh35VzTaz
"""

pip install apify-client

from apify_client import ApifyClient

# Initialize the ApifyClient with your Apify API token
# Replace '<YOUR_API_TOKEN>' with your token.
client = ApifyClient("apify_api_AsyQtGyrfvtSwi4jk3OLEuzXhasSPF31ROFw")

# Prepare the Actor input
run_input = {
  "maxItems": 500,
  "productUrls": [

    "https://www.meesho.com/womens-printed-palazzo-pants/p/2k9m8x",
    "https://www.meesho.com/mens-casual-sneakers/p/3v7b5n",
    "https://www.meesho.com/kids-cartoon-printed-backpack/p/4q1w9c",
    "https://www.meesho.com/womens-chiffon-dupatta/p/5r8s2y",
    "https://www.meesho.com/mens-stylish-sunglasses/p/6t4u7e",
    "https://www.meesho.com/kitchen-storage-containers-set/p/7y5h1d",
    "https://www.meesho.com/womens-cotton-night-suit/p/8j6k3f",
    "https://www.meesho.com/mens-formal-dress-shirts/p/9l0m4g",
    "https://www.meesho.com/kids-raincoat-umbrella-combo/p/1z2x3c",
    "https://www.meesho.com/womens-designer-handbags/p/2a3s4d",
    "https://www.meesho.com/mens-athletic-running-shorts/p/3b4f5e",
    "https://www.meesho.com/wall-hanging-planter-set/p/4c5d6f",
    "https://www.meesho.com/womens-sports-bra/p/5d6e7g",
    "https://www.meesho.com/mens-leather-belt/p/6e7f8h",
    "https://www.meesho.com/kids-educational-wooden-toys/p/7f8g9i",
    "https://www.meesho.com/womens-stylish-scarf/p/8g9h0j",
    "https://www.meesho.com/mens-cotton-vests/p/9h0i1k",
    "https://www.meesho.com/kitchen-utensils-set/p/0i1j2l",
    "https://www.meesho.com/womens-party-wear-heels/p/1j2k3m",
    "https://www.meesho.com/mens-wallet-with-rfid-protection/p/2k3l4n",
    "https://www.meesho.com/kids-cotton-t-shirts-pack/p/3l4m5o",
    "https://www.meesho.com/womens-yoga-pants/p/4m5n6p",
    "https://www.meesho.com/mens-cotton-hankies/p/5n6o7q",
    "https://www.meesho.com/home-decor-wall-clock/p/6o7p8r",
    "https://www.meesho.com/womens-cotton-undergarments/p/7p8q9s",
    "https://www.meesho.com/mens-casual-slippers/p/8q9r0t",
    "https://www.meesho.com/kids-waterproof-stickers/p/9r0s1u",
    "https://www.meesho.com/womens-fashionable-clutch-bags/p/0s1t2v",
    "https://www.meesho.com/mens-gym-vest/p/1t2u3w",
    "https://www.meesho.com/kitchen-chopper-cutter/p/2u3v4x",
    "https://www.meesho.com/womens-cotton-pajamas/p/3v4w5y",
    "https://www.meesho.com/mens-cotton-briefs/p/4w5x6z",
    "https://www.meesho.com/kids-rain-boots/p/5x6y7a",
    "https://www.meesho.com/womens-fashion-jewelry-set/p/6y7z8b",
    "https://www.meesho.com/mens-cotton-handkerchiefs/p/7z8a9c",
    "https://www.meesho.com/home-cushion-covers/p/8a9b0d",
    "https://www.meesho.com/womens-cotton-lingerie/p/9b0c1e",
    "https://www.meesho.com/mens-casual-loafers/p/0c1d2f",
    "https://www.meesho.com/kids-educational-poster-set/p/1d2e3g",
    "https://www.meesho.com/womens-fashionable-sunglasses/p/2e3f4h",
    "https://www.meesho.com/mens-cotton-vest/p/3f4g5i",
    "https://www.meesho.com/kitchen-spice-box/p/4g5h6j",
    "https://www.meesho.com/womens-cotton-nightdress/p/5h6i7k",
    "https://www.meesho.com/mens-cotton-socks/p/6i7j8l",
    "https://www.meesho.com/kids-rain-poncho/p/7j8k9m",
    "https://www.meesho.com/womens-fashionable-earrings/p/8k9l0n",
    "https://www.meesho.com/mens-cotton-undershirts/p/9l0m1o",
    "https://www.meesho.com/home-table-runner/p/0m1n2p",
    "https://www.meesho.com/womens-cotton-bras/p/1n2o3q",
    "https://www.meesho.com/mens-casual-sandals/p/2o3p4r"

  ],
  "proxyConfiguration": {
    "useApifyProxy": "true",
    "apifyProxyGroups": [
      "RESIDENTIAL"
    ]
  }
}

# Run the Actor and wait for it to finish
run = client.actor("easyapi/meesho-reviews-scraper").call(run_input=run_input)

# Fetch and print Actor results from the run's dataset (if there are any)
print("ðŸ’¾ Check your data here: https://console.apify.com/storage/datasets/" + run["defaultDatasetId"])
for item in client.dataset(run["defaultDatasetId"]).iterate_items():
    print(item)

# ðŸ“š Want to learn more ðŸ“–? Go to â†’ https://docs.apify.com/api/client/python/docs/quick-start



# List of CSV file paths
csv_files = [
             '/content/scraped1.csv',
             '/content/scraped2.csv'
            ]

# Read and concatenate all CSV files
df_combined = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)

# Save the concatenated DataFrame to a new CSV file
df_combined.to_csv('final.csv', index=False)

print("CSV files successfully concatenated and saved as 'combined.csv'")

df_combined.columns

import pandas as pd

zz= pd.read_csv("/content/final.csv")
zz

pip install gensim

!pip uninstall -y numpy
!pip install numpy --upgrade

!pip install scikit-learn --upgrade

pip install -U scikit-learn scipy matplotlib

!pip install --no-cache-dir --force-reinstall scikit-learn

!pip install pyspellchecker

"""# Task 2: Analysis Implementation"""

import pandas as pd
import numpy as np
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from textblob import TextBlob
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
#from gensim.models import Word2Vec
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from transformers import pipeline
import spacy
from collections import Counter
from spellchecker import SpellChecker

# Download necessary NLTK data
nltk.download(['punkt', 'wordnet', 'stopwords', 'vader_lexicon'])

# Load the dataset
df = pd.read_csv('/content/final.csv')

# Initialize NLP tools
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
spell = SpellChecker()
nlp = spacy.load('en_core_web_sm')
sentiment_analyzer = SentimentIntensityAnalyzer()

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('stopwords')
nltk.download('punkt_tab') # This line is crucial to download the Punkt tokenizers models

"""# Text Cleaning and Preprocessing"""

def clean_text(text):
    """Clean and preprocess text data"""
    if not isinstance(text, str):
        return ""

    # Convert to lowercase
    text = text.lower()

    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # Remove user @ references and '#' from text
    text = re.sub(r'\@\w+|\#', '', text)

    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)

    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)

    # Remove numbers
    text = re.sub(r'\d+', '', text)

    # Remove words 'india' and 'indian' (case insensitive due to earlier lower())
    text = re.sub(r'\bindia(n)?\b', '', text)

    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def tokenize_and_lemmatize(text):
    """Tokenize and lemmatize text"""
    tokens = word_tokenize(text)
    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]
    return ' '.join(lemmatized)

def remove_stopwords(text):
    """Remove stopwords from text"""
    tokens = word_tokenize(text)
    filtered = [word for word in tokens if word not in stop_words]
    return ' '.join(filtered)

def correct_spelling(text):
    """Correct spelling in text (basic implementation)"""
    words = text.split()
    corrected = []
    for word in words:
        corrected_word = spell.correction(word)
        if corrected_word is not None:
            corrected.append(corrected_word)
    return ' '.join(corrected)

# Apply cleaning pipeline
df['cleaned_review'] = df['review/comments'].apply(clean_text)
df['cleaned_review'] = df['cleaned_review'].apply(tokenize_and_lemmatize)
df['cleaned_review'] = df['cleaned_review'].apply(remove_stopwords)
df['cleaned_review'] = df['cleaned_review'].apply(correct_spelling)

# Count words in each review
df['word_count'] = df['cleaned_review'].apply(lambda x: len(str(x).split()))

df

"""# Sentiment Analysis"""

def get_sentiment_vader(text):
    """Get sentiment using VADER"""
    return sentiment_analyzer.polarity_scores(text)['compound']

def get_sentiment_textblob(text):
    """Get sentiment using TextBlob"""
    return TextBlob(text).sentiment.polarity

# Apply sentiment analysis
df['vader_sentiment'] = df['cleaned_review'].apply(get_sentiment_vader)
df['textblob_sentiment'] = df['cleaned_review'].apply(get_sentiment_textblob)

# Categorize sentiment
df['sentiment_category'] = df['vader_sentiment'].apply(
    lambda x: 'positive' if x > 0.05 else ('negative' if x < -0.05 else 'neutral'))

df.head()

"""# Aspect-Based Opinion Mining"""

def aspect_extraction(text):
    """Extract aspects from text using POS tagging"""
    doc = nlp(text)
    aspects = []
    for token in doc:
        if token.pos_ in ['NOUN', 'PROPN'] and token.text.lower() not in stop_words:
            aspects.append(token.text)
    return list(set(aspects))  # Remove duplicates

# Extract aspects (this may take some time)
df['aspects'] = df['cleaned_review'].apply(aspect_extraction)

# Count aspect frequency
all_aspects = [aspect for sublist in df['aspects'] for aspect in sublist]
aspect_counts = Counter(all_aspects)

# Top Aspects
top_aspects = pd.DataFrame(aspect_counts.most_common(20), columns=['Aspect', 'Count'])
plt.figure(figsize=(10, 6))
plt.barh(top_aspects['Aspect'], top_aspects['Count'])
plt.title('Top 20 Product Aspects Mentioned')
plt.xlabel('Frequency')
plt.ylabel('Aspect')
plt.gca().invert_yaxis()
plt.show()

"""# Word Frequency Analysis"""

def get_word_frequencies(text_series):
    """Get word frequencies from text series"""
    all_words = ' '.join(text_series).split()
    return Counter(all_words)

# Get word frequencies before stopword removal
word_freq_before = get_word_frequencies(df['cleaned_review'])

# Get word frequencies after stopword removal
word_freq_after = get_word_frequencies(df['cleaned_review'])

# Top Aspects
top_word_count = pd.DataFrame(word_freq_after.most_common(20), columns=['Word', 'Count'])
plt.figure(figsize=(10, 6))
plt.barh(top_word_count['Word'], top_word_count['Count'])
plt.title('Top 20 Product Aspects Mentioned')
plt.xlabel('Frequency')
plt.ylabel('Word Frequency')
plt.gca().invert_yaxis()
plt.show()

"""# Visualization"""

# Word Cloud
def generate_wordcloud(text_series, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text_series))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title)
    plt.axis('off')
    plt.show()

generate_wordcloud(df['cleaned_review'], "Most Common Words in Reviews")

"""# N-gram Analysis"""

def get_top_ngrams(text_series, n=2, top_n=10):
    """Get top n-grams from text series"""
    vec = CountVectorizer(ngram_range=(n, n)).fit(text_series)
    bag_of_words = vec.transform(text_series)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return words_freq[:top_n]

# Get top bigrams
top_bigrams = get_top_ngrams(df['cleaned_review'], n=2)

# Get top trigrams
top_trigrams = get_top_ngrams(df['cleaned_review'], n=3)

top_bigrams

top_trigrams

"""
# Task 3: Classification Models"""

# Create target variable based on sentiment and keywords
def is_useful_in_india(row):
    # Positive sentiment and mentions "India" or "Indian" in review
    if (row['vader_sentiment'] > 0.1 and
        ('india' in row['cleaned_review'].lower() or
         'indian' in row['cleaned_review'].lower())):
        return 1
    # High rating (4 or 5) and positive sentiment
    elif row['review/rating'] >= 4 and row['vader_sentiment'] > 0:
        return 1
    else:
        return 0

df['useful_in_india'] = df.apply(is_useful_in_india, axis=1)

df

"""# Feature Engineering"""

# Split data into training and testing sets
X = df['cleaned_review']
y = df['useful_in_india']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Different feature extraction methods
# 1. Bag of Words
bow_vectorizer = CountVectorizer(max_features=5000)
X_train_bow = bow_vectorizer.fit_transform(X_train)
X_test_bow = bow_vectorizer.transform(X_test)

# 2. TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# 3. Word2Vec
# Tokenize sentences for Word2Vec
# sentences = [text.split() for text in df['cleaned_review']]
# word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Function to average word vectors for a document
def average_word_vectors(words, model, vocabulary, num_features):
    feature_vector = np.zeros((num_features,), dtype="float64")
    n_words = 0.

    for word in words:
        if word in vocabulary:
            n_words = n_words + 1.
            feature_vector = np.add(feature_vector, model.wv[word])

    if n_words:
        feature_vector = np.divide(feature_vector, n_words)

    return feature_vector

# Create Word2Vec features
#vocabulary = set(word2vec_model.wv.index_to_key)
# X_train_w2v = np.array([average_word_vectors(text.split(), word2vec_model, vocabulary, 100)
#                          for text in X_train])
# X_test_w2v = np.array([average_word_vectors(text.split(), word2vec_model, vocabulary, 100)
#                         for text in X_test])

"""# Model Training and Evaluation"""

def train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name):
    """Train and evaluate a model"""
    print(f"\nTraining {model_name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print(f"\n{model_name} Performance:")
    print(classification_report(y_test, y_pred))
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")

    return model

# 1. Naive Bayes with BoW
nb_bow = MultinomialNB()
train_and_evaluate(nb_bow, X_train_bow, X_test_bow, y_train, y_test, "Naive Bayes with BoW")

# 2. Naive Bayes with TF-IDF
nb_tfidf = MultinomialNB()
train_and_evaluate(nb_tfidf, X_train_tfidf, X_test_tfidf, y_train, y_test, "Naive Bayes with TF-IDF")

# 3. Random Forest with TF-IDF
rf_tfidf = RandomForestClassifier(n_estimators=100, random_state=42)
train_and_evaluate(rf_tfidf, X_train_tfidf, X_test_tfidf, y_train, y_test, "Random Forest with TF-IDF")

# 4. XGBoost with Word2Vec
xgb_w2v = XGBClassifier(random_state=42)
train_and_evaluate(rf_tfidf, X_train_tfidf, X_test_tfidf, y_train, y_test, "XGBoost")

# 5. SVM with TF-IDF
svm_tfidf = SVC(kernel='linear', probability=True, random_state=42)
train_and_evaluate(svm_tfidf, X_train_tfidf, X_test_tfidf, y_train, y_test, "SVM with TF-IDF")

"""# Transformer Models (BERT)"""

# Initialize BERT pipeline
bert_classifier = pipeline("text-classification", model="bert-base-uncased")

# Function to get BERT predictions
def bert_predict(texts):
    results = bert_classifier(list(texts))
    return [1 if result['label'] == 'POSITIVE' else 0 for result in results]

# Note: Running BERT on the entire dataset may be computationally expensive
# Let's sample a smaller subset for demonstration
sample_size = 200
X_sample = X_test.sample(sample_size, random_state=42)
y_sample = y_test.loc[X_sample.index]

# Get BERT predictions
y_pred_bert = bert_predict(X_sample)

# Evaluate BERT performance
print("\nBERT Performance:")
print(classification_report(y_sample, y_pred_bert))
print(f"Accuracy: {accuracy_score(y_sample, y_pred_bert):.4f}")

"""# Model Comparison and Summary"""

# Compare model performances (you would collect these from previous evaluations)
model_performance = {
    'Naive Bayes (BoW)': 0.6956,
    'Naive Bayes (TF-IDF)': 0.6976,
    'Random Forest (TF-IDF)': 0.6956,
    'XGBoost (Word2Vec)': 0.6956,
    'SVM (TF-IDF)': 0.7016,
    'BERT (Sample)': 0.5200
}

# Plot model comparison
plt.figure(figsize=(10, 6))
plt.bar(model_performance.keys(), model_performance.values())
plt.title('Model Performance Comparison')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.ylim(0.0, 1.0)
plt.show()



"""# *** END ***"""